# What is it?
Purpose of kinesis firehose is to store data from kinesis streams directly in targets.

Firehose stores data in bigger batches, instead of streaming. It means data availability is near-real-time.

* fully managed service, so it doesn't require any administration
* near real time, minimum 60 seconds latency due to way how it handles data (batching)
* automatic scalling
* supports many data formats
* possible data conversion from json to parquet or manual conversion using AWS Lambda
* support compession GZIP, ZIP or Snappy (when target is amazon S3) or GZIP (Redshift)
* possible to store all source data on separate S3 bucket (to be able to see source history, ie. in case of transformation failures)
* costs are calculated on amount of data (so if there is no source data, you pay nothing )

# Possible Destinations
* AWS S3
* AWS Redshift (through S3 COPY)
* AWS ElasticSearch
* datadog
* mongodb
* splunk ()
* (possible other new 3rd party partners)
* custom destinations - via http endpoint (API)

# Firehose Buffer Sizing
* firehose accumulate records in a buffer
* buffer is flushed based on time and size rules
* it means data are stored on target when buffer size or butter time is exceeded
* firehose can automatically increase buffer size to increase throughput

there is a minimum limit for buffer time - equal 1 minute

# Comparison to data streams consumer
* streams data is focused on processing messages, where firehose is focused on data ingestion and delivery
* streams stores data for certain period, while firehose doesn't store data at all - it sends data to target destinations
* firehose has builtin functionality of implementing lambda for processing data, in streams you have to implement it by yourself
* firehose is near-real-time (>1 minute latency) vs streams is real-time (>200ms of latency)

