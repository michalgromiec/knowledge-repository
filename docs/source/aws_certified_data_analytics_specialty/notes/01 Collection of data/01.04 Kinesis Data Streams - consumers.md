# Kinesis Consumers
Used for getting data from stream to clients/apps

Types of Kinesis Consumers:
* SDK - you can use AWS CLI to read records from Kinesis Stream
* KCL - Kinesis Consumer Library, have more option than SDK and are able to solve advanced issues
* Kinesis Collector Library
* AWS Lambda
* 3rd party applications (Spark - even directly, Kafka, NiFi)
* Kinesis Firehose

Values of kinesis streams records data are base64 encoded - therefore consumers has to encode them to see proper values.  

## AWS SDK

### Classic/Standard
* you can read/poll record from stream using `get-records` (many records) cli command
* GetRecords polls records from Kinesis Stream with limits for one poll:
  * up to 2MB/sec for each shard (checked each 5 seconds - see below) 
  * up to 10MB of data (then throttle for 5 seconds)
  * up to 10.000 records (then throttle for 5 seconds)
  * up to 5 executions/second for GetRecords - means you have minimum 200ms latency of data
  * **limits are shared by all consumers** - means if you have i.e. 10 consumers consuming from the same shard, they have shared limit of 2MB/sec
* above limits are shared by all consumers for shard - meaning if you have more consumers which polls data from one shard, their latency will be higher because they will share one 10MB/5sec limit and 5 executions/second

### Fan-out enhanced (fan out)
* implemented in August 2018
* works with KCL 2.0 and AWS Lambda
* in opposition to classic, each consumer gets own limit of data transfered - 2MB per shard
* in opposition to classic, consumer not polling data, but subscribe to shard. After subscription, Kinesis **pushes** data to consumer over HTTP/2. 
* easier scaling out - you can add more consumers which transforms data from kinesis stream (still there is a limit for producing data - 1MB/s, but can be helpful when consumer stopped work and after recover needs to eat the lag and transform records send to kinesis when consumer didn't work)
* has additional costs, but throughput is increased and latency is decreased to ~70ms

USE-CASE:
* standard:
  * you can tolerate ~200ms latency
  * low number of consuming application
  * minimize costs
* fan out consumers:
  * low latency requirements (~70ms)
  * multiple consumers for the same stream
  * can accept higher costs

## AWS KCL
KCL = Kinesis Client Library
* java-first library, but created also for other languages (python, go, ruby, node)
* can read records from Kinesis Stream produced with KPL (can de-aggregate records created by KPL)
* share multiple shards with multiple consumers (and aggregates their limits)
* able to checkpointing to resume progress in case of application downtime
* uses DynamoDB below for coordination and checkpointing (one row per shard)
* in case of ExpiredIteratorException raised, you need to increase WCU (write capacity units) in your dynamo table setup

## AWS Kinesis Connector Library
older java library
* reads data from Kinesis Stream and send them to other services like S3, DynamoDB, Redshift or Elasticsearch Service
* in most of areas replaced by Kinesis Firehose or AWS Lambda
* must be running on EC2 instance

## AWS Lambda
* can read records from Kinesis Stream
* can de-aggregate records created by KPL (by additional small library)
* can be used to lightweight ETL and saves data to:
  * S3
  * DynamoDB
  * Redshift
  * ElasticSearch
  * other, almost not limited as long as implemented in programming language used in Lambda function
* can be used to trigger notification (SNS)
* has a configurable batch size (you can declare how many records has to be waited to execute Lambda function and increase throughput)


# Hands-on
## AWS SDK (CLI)
### Stream Management
```commandline
aws kinesis create-stream --stream-name test_mg --shard-count 1 --profile michal --region eu-west-1
aws kinesis list-streams --profile michal --region eu-west-1
aws kinesis describe-stream --stream-name test_mg --region eu-west-1 --profile michal
aws kinesis update-shard-count --stream-name test_mg --target-shard-count 2 --scaling_type UNIFORM_SCALING
aws kinesis delete-stream --stream-name test_mg --profile michal --region eu-west-1
```
### Consumer
**get-shard-iterator** is used to get current position of shard, option `TRIM_HORIZON` used here is to specify that shard data should be returned from the oldest data in the shard.
Other types are:
* LATEST - points to the newest data, allows to read only new data added after shard iterator is obtained
* AT_TIMESTAMP - points to data at specific time
* AFTER_SEQUENCE_NUMBER - points to data after specific sequence number

Function `get-records` in response returns NextShardIterator next to data - so you can use it in consuming next records from stream

```commandline
aws kinesis get-shard-iterator --stream-name test_mg --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON --profile michal --region eu-west-1
aws kinesis get-records --shard-iterator "AAAAAAAAAAGqN+Z5rHgWgExCnlJa0txKc/95odt/jhTh6GtAwBS32R6QazWe8BMCBfcF7u3Q7wax+7y3avWcYLpw10lt+jDrJitVnmnQwcL3mTf9QATnv/hsFFUOb9uELHADYNsLM2Ee1X1mpkVEKewrUlswpEu3gBqbadqlZ9i+Vu44Ygyuf0SRhmvMDk9THfq9QgthynomGlumQmR6nmEjQCJPfzPbDg4O4hRbL2GkSK2n2wrjGA==" --profile michal --region eu-west-1
```

**when using SDK, you have to manually put shard id in command, in opposition to KCL when this option is managed automatically**
