# Amazon Athena
query engine for doing interactive queries of S3 data

## What is it
* completely serverless (pay for scanned data)
* interactive query service for S3 (data stays at S3, Athena give only interface and optimizer for this data)
* under the hood - Presto
* supports many data formats
  * CSV
  * JSON
  * ORC (columnar, splittable)
  * Parquet (columnar, splittable)
  * Avro (splittable)
* can read unstructured, semi-structured or structured data
* works with Glue Data Catalog

## Federated queries

Originally Athena was able to query **data only from S3 buckets**, but AWS add ability to connect other data sources (Redshift, MySQL, Postgres, HBase, DynamoDB, other JDBC-compliant sources...).

In practice, AWS Lambda is being created and Athena query this lambda, which is responsible for translate data from data sources to Athena.

Using this process you can also get access to external hive metastore.

## Use cases
* ad-hoc queries of web logs
* querying staging data before loading to redshift
* querying historical, less used data and combine with hot data to serve all data to end user
* analyze cloudtrail/cloudfront/application logs in S3
* integration with Jupyter/Zeppelin/RStudio notebooks
* integration with QuickSight
* integration via ODBC/JDBC with other visualization tools

## Integration with AWS Glue
* uses AWS Glue Data Catalog by default (it is possible to add other data source by using Lambda's functions, supported heavily by AWS GUI)
* Athena is based on Presto, so it doesn't contain table schemas directly - it uses external data sources, in this example - Hive metastore which is in this definition AWS Glue Meta Store

## Workgroups
* can organize users/teams/apps into Workgroups
* you can control query access and track costs by Workgroup
* integrates with IAM/Cloudwatch/SNS
* workgroups have its own separate:
  * query history (via S3 path set for query history)
  * data limits (limit how much data queries may scan by workgroup)
  * IAM policies
  * Encryption settings

## Pricing model
* pay as you go
* $5 per TB scanned
* successful or cancelled queries count, failed queries do not
* all DDL operations are free (CREATE/ALTER/DROP)
* you can save lots of money by using columnar formats (ORC/Parquet, by allowing athena to query only necessary columns not all data and by reucing size of column by columnar storage)
* schema partitioning also reduce size of scanned data

## Security
* access control by IAM (access to athena, read from S3) and S3 bucket policies (permissions for certain path in S3 bucket)
* encrypt results at rest in S3 staging directory
  * SSE-S3, SSE-KMS and CSE-KMS (being honest, it's an option of S3 not Athena)
* in-transit secured by TLS (HTTPS), also between Athena and S3

## Anti-patterns
* visualization, highly formatted reports
* ETL - use Glue instead, EMR with Apache Spark

## Performance optimization
* use columnar data
* small number of large files performs better than large number of small files
* use partitions

## ACID transactions
* powered by Apache Iceberg (just add `table_type='ICEBERG` in TBLPROPERTIES of CREATE TABLE command)
* compatible with EMR/Spark and any other tool supporting Iceberg table format
* implement record locking (to prevent from changing data when more than one user modifies record in the same moment)
* allows for time travel operations
  * user is able to revoer data recently deleted
* need to manual optimize table to prevent from degrade performance of table (by using `OPTIMIZE` command)
 