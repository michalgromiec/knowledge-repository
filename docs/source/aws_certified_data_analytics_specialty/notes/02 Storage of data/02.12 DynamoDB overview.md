# AWS DynamoDB
* fully managed no-sql database service, serverless, 
* highly available with replication across multiple AZs
* scales to massive workloads, distributed database
* able to perform millions of requests per seconds, trillions of row, TBs of storage
* fast and consistent in performance (low latency on retrieval)
* integrated with IAM for security, authorization and administration
* low cost and autoscaling capabilities

## What are nosql databases
* non-relational databases, **distributed**
* examples: mongodb, dynamodb, cassandra
* do not support joins (or very limited support)
* do not support aggregation functions
* all data needed for query has to be in one record
* able to **scale horizontally**
* schema on write (able to store data without having to specify a fixed schema upfront)

## DynamoDB basics
* dynamodb is made of Tables
* Table has a Primary Key (defined at table creation time)
* Table can have infinite number of Items (called also rows, records)
* Each Item has attributes (columns, fields, can be added over time thanks to write-on-schema)
* Maximum size of item is 400KB
* Supported data types:
  * scalar types: string, number, minary, boolean, null
  * document types: list, map
  * set types: string set, number set, binary set
* as an example of nosql database, doesnt support constraints like not, foreign key, unique index etc

## DynamoDB primary key
As you have to define primary key on table creation, you have to choose the best one primary key.

You should **choose field with the highest cardinality and not to choose skewed distributed field**.
Primary key has to be not-null.

### Partition Key (HASH)
Like in rdbs, **primary key must be unique** for each item

### Partition Key + Sort Key (HASH+RANGE)
The **combination of partition key and sort key must be unique** for each item (so partition key can be repeated for more than one item)
Example: user and game with highscores.

## Table class
* standard = default
* standard-ia = infrequently accessed, cost optimized

## Use cases in Bigdata world
* mobile apps, gaming
* digital ad serving
* live voting
* audience interaction for live events
* sensor networks
* log ingestion
* metadata storage for amazon S3 objects
* web session management
* in general - for small hot data

Antipattern:
* need of relation joins
* blob data
* complex transactions
* low i/o rate

## Consistency
### Eventually Consistent Read
reads data no matter if it's replicated in full between DynamoDB replicas server (underneath, not visible for users)

```{note}
One Eventually Consistent Read consumes half of the RCU, so it's cheaper to use ventual consistency
```
### Strongly Consistent Read
reads only those data which are replicated in full between replicas server

## Capacity Units
### Read Capacity Units - throughput for reads
* 1 RCU = 1 strongly consistent read or 2 eventually consistent reads per second for an item up to 4KB in size
* example = 7 strongly consistent read per second, each 2.7KB in size = 7 * ROUNDUP(2.7/4) = 7RCU
* example = 11 eventually consistent  reads per second for an item 3.7KB in size = 11/2 * ROUNDUP(3.7/4) = 5.5RCU
### Write Capacity Units - throughput for writes
* 1 WCU = 1 write per second for an item up to 1KB in size
* items larger than 1KB, more WCUs are consumed
* example = 7 items per second, each 2.7KB in size => 7*ROUNDUP(2.7/1) = 21WCU
* ```{warning}delete operations on nonexistent items consumes one write capacity unit```

## DynamoDB partitions - internal details
* when you write item to dynamodb, dynamodb calculates hash for primary key of written item and based on hash function result decide to which partition record will be written
* hot partition = partition for which the most items are written (mostly because of primary key, not because of hashing function)
* number of partitions depends on provisioned throughput and the size of the items in the table
  * the larger provisioned throughput or larger items in table = the more partitions dynamodb creates to store the data
  * number of partitions is calculated as higher number of two below numbers:
    * RCU_total/3000 + WCU_total/1000
    * table_size_total/10GB
* dynamodb **automatically scales the number of partitions to meet the capacity requirements**
* when number of partitions change, data in the table is redistributed across the new partitions (process known as **partition split**). The aim is to evenly distribute data across the partitions, to ensure the highest performance of the table
* partition split can result in increased read and write lantecies (yet it doesn't consume capacity units, it's a background process)

## Capacity modes
There are two modes of capacity/throughput, you can switch between then once every 24 hours

### Provision Mode
* default (10RCU/10WCU)
* specify the number of reads/writes per seconds
* need to plan capacity beforehand
* pay for **provision** (not only used) read & write capacity units
* option to set up autoscaling of throughput to meet demand (specify minimum and maximum capacity units and target utilization - required capacity units number is calculated based on target utilization percentage)
* limits can be exceeded temporarily using **Burst Capacity**
* if Burst Capacity consumed, `ProvisionedThroughtputExceededException` is raised - in this case retry is needed to store/read data

### On-Demand Mode
* automatically scale read/writes
* no capacity planning
* pay for what you use, more often more expensive
* no throttle - pay for everything you want to do
* you are charged for reads/writes that you use in terms of RRU Read Request Units/WRU Write Request Units (RRU = RCU, WRU = WCU)
* 2.5x **more expensive than provisioned capacity**
* use cases: 
  * unknown workloads, 
  * unpredictable application traffic,
  * quick data migration on start (very high number of writes in short time - you dont need to set very high WCU capacity for 24 hours)

## Types of indexes
as in rdbs, there is an option to create indexes which are supplementary to primary key and sort key
### Local Secondary Index (LSI)
* alternative sort key for table, primary key stay the same
* consist of one scalar attribute (string,number,binary)
* table can have up to 5 LSIs
* must be defined at table creation time (even though attribute is not visible - )
* attribute projections - results can contain some or all attributes of the table, but also ca contain keys only, so you can retrieve keys only and send next getitem request for selected keys

### Global Secondary Index (GSI)
* alternative primary key
* speed up queries on non-key attributes
* must provision separate RCUs & WCUs for the index
* allows to query data by non-primary-key attribute (yet still data are the same and are returned as it would be basic table)
* can be added/modified after the table creation

### Indexes impact on main table throttle
* GSI
  * **if the writes are throthled on the GSI, the main table will be throttled also** (even if the WCU on the main table are fine)
* LSI
  * uses main table RCUs/WCUs, so no special impact on throthling

## DynamoDB PartiQL
PartiQL is a query service which allow to use a SQL-like syntax to manipulate DynamoDB tables

It supports some statements of CRUD (Insert/update/select/delete).

In practise, it works similar to i.e. Athena Query Editor - there is a query editor in GUI available for users.

It has been introduced on 2020-11-30.

Using partiql you can do very basic queries.

## DynamoDB DAX
DAX (DynamoDB Accelerator) is a separate cluster contains cache nodes for dynamodb. It's fully-managed, highly available, seamless in-memory.

* allows for microseconds latency for cached queries.
* doesn't require any change in application, simply recognize that query has been cached and return cached results.
* solves "hot key" problem - when reading same key very often, you would consume a lot of RCUs without DAX (cache)
* default cache TTL is 300 seconds (can be changed)

You can have up to 10 nodes in the cluster, it's possible to have multi-az environment (min 3 nodes recommended).
It's up to developer how big cluster will be, he can also decide what types of machines will be used to create cluster (it's quite detailed, you have to create VPC, subnet, security groups, so it seems that underneath group of EC2 containers are created).

in effect, it's quite expensive, as it's not serverless and the smallest cluster could cost hundreds of dollars monthly (on the other hand, RCU capacity can be decreased if your dynamodb tables are frequently requested for reading the same data)

to use dax cluster, application should query dax cluster endpoint instead of dynamodb endpoint

### Differences vs ElastiCache
* DAX has been build specifically for dynamodb, elasticache supports other data stores, like redis, memcached
* DAX is run on cluster and have native replication
* DAX is optimized for read-heavy workloads
* DAX have a cache of individual objects (queries/scans), while elasticache stores whole computational process (aggregation etc)

## DynamoDB streams
dynamodb table stream is the list of all modifications made to dynamodb table.

so as an effect it is something like change log for all items, and it can be:
* sent to kinesis data streams
* sent to lambda
* read by kinesis client library applications

Data Retention is up to 24 hours

You can choose what data will be sent to target:
* KEYS_ONLY - only the key attributes of the modified item
* NEW_IMAGE - entire item after changes
* OLD_IMAGE - entire item before changes
* NEW_AND_OLD_IMAGES - both entire items before and after changes

dynamodb streams are very similar to kinesis - e.g. it is made of shards.

stream is populated with changes made only after stream has been enabled (can't create list of historical changes) 

### Use Cases
* notification for users (i.e. lambda verifying changes on dynamodb table and sending mail, or SES on SNS on kinesis data stream)
* analytics (firehose on top on kinesis stream and archiving data in amazon s3)
* workaround for triggers (lambda inserts data into other tables based on changes in main table)

## DynamoDB items TTL
TTL = Time To Live, automatically delete items after an expiry timestamp

* doesn't consume any WCU
* in practice, you need to have attribute which will contain epoch timestamp of item expiration timestamp and select this attribute in table Time To Live configuration
* expired items are deleted within maximum 48 hours after expiration
* expired but not yet deleted items still appears in query results (has to be excluded manually using filters)
* items are dropped also from LSI and GSI and operation is also visible in stream
* use case: session data

## Integrations with AWS S3
### BLOBs (Large Objects Pattern)
one example of interface with S3 is when your item contains blob greater than maximum dynamodb item size.
In such case - you upload blob to s3 and store in dynamodb table url to this object.

### Indexing S3 Objects Metadata
1. application upload object to S3 bucket
2. bucket event notification invokes lambda, which stores object metadata (size, dates, who created) in dynamodb
3. application can query dynamodb to retrieve only metadata of objects
3b. client can have access to metadata only, without access to s3
3c. client can query objects by metadata easily (i.e. by partiql queries)

## Security
* dynamodb tables can be accessed by vpc endpoints
* access is fully controlled by iam rules
* encryption at rest using SSE-KMS
* encryption in transit using SSL/TLS

## Backup and restore
* possible restore to point in time like in RDS
* possible migration to other types of databases using AWS DMS (data migration service)

## Efficiency
* global tables which are multiregional, fully replicated

## API Commands
### PutItem
* Creates or update (upsert) item
* update in full - source item are stored in full in dynamodb table

### UpdateItem
* edits existing item or adds a new item if it doesn't exist
* update only source attributes
* can be used to implement atomic counters

### Conditional Writes
* do a write/update/delete only if conditions are met
* helps with concurrent access to items

### GetItem
* read item on primary Key (HASH or HASH+RANGE)
* by default - eventually consistent read, option to use Strongly Consistent Read
* option ProjectionExpression - specifies attributes to return by dynamodb, optimizes network overflow

### Query
* returns **list of items** based on
  * KeyConditionExpression - partition key exact value and sort key (equal, higher than, lower then, between, begins and other)
  * FilterExpression - additional filtering after the Query operation (can use non-key attributes)
  * return number of items specified in **Limit or up to 1MB of data**
  * ability to do pagination on the results
  
### Scan
* scan **entire table** and then filter out data, but **on the client side** (very inefficient)
* consumes a lot of RCU
* returns up to 1MB of data - pagination possible
* use case: export entire table
* can use ProjectionExpression & FilterExpression (but it doesn't have impact on very high RCU consumption)

### DeleteItem
* delete individual item
* conditional delete possible

### DeleteTable
* similar to sql truncate table
* deletes a whole table and all items without reading all items and deleting them one-by-one

### BatchWriteItem
* up to 25 PutItem and DeleteItem in one call
* up to 16MB of data written, up to 400KB of data per item
* reduces latency of API calls, but WCU consumption are the same
* not atomic as a whole, if some items are rejected (i.e. because provisional throughput exceeded), failed operations returned in the `UnprocessedItems` response attribute.

### BatchGetItem
* return items from one or more tables
* returns up to 100 items, up to 16MB of data
* items retrieved in parallel to minimize latency

