* amazon S3 automatically scales to high request rates with latency 100-200ms
* s3 allows for 3500 PUT/COPY/POST/DELETE and 5500 GET/HEAD operations per second per prefix (perfix = full path to file)


## Multipart upload
* recommended for files > 100MB, required for files > 5GB (single S3 Put have hard limit of 5GB of object size)
* in practice, big file is split and paralelly uploaded to S3, and when all of them are uploaded to S3, S3 concatenate them into one big file and store it like it was one big file
* done by CLI under the hood, but can be done also manually, especially when SDK is used:
  * Create Multpart Upload (CreateMultipartUpload API call, returns Bucket, Key, UploadID)
  * Upload Parts (UploadPart API call, user needs to provide Bucket, Key, **Part Number** and UploadID returned from create mulipart upload, returns ETag). Part Number is important so AWS would know in what order parts should be merged when multipart upload is finished
  * Complete Multipart Upload (CompleteMultipartUpload API call, tell that uploads are finished, provide bucket, key, uploadid and all part numbers and etags)
* to gain the highest performance, use multiprocessing or threading when you do multipart upload using boto3 () 


Limitations:
* up to 10000 parts only
* mistake leads to overwrite parts when same part number is used
* can be lifecycle policy which closes multipart uploads after a specified time period
* all parts except the final part must be at least 5MB (so it's not possible to multipart upload for objects smaller than 5MB)

## S3 Transfer Acceleration
* increases transfer speed (mostly upload)
* transfering data to one of hundreds edge location using public internet, and therefore AWS copies data between regions by using very fast AWS private network
* enabled per bucket
* to use acceleration, user have to send request to separate endpoint (changed in application - boto3 or cli)
* generate additional costs (0.04$/GB of transfered data and 0.08$ outside US, Europe and Japan). Costs are calculated only when acceleearted data is faster than a standard data transfer
* It is compatible with multi-part upload.
* for the issue with download speed, cloudfront cache should be used

## S3 byte-range fetches
similar to multi-part upload, allows to get range of object (i.e. allows to get only head 50 bytes of object)